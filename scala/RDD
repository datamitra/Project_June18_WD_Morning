export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
spark-shell
val order=sc.textFile("/user/hive/warehouse/hdp_datalake.db/orderdetails/part-m-00000")
order.first
order.map(_.split("\u0001")).first
#Read by column names from RDD??

case class orderline(ordernumber:Int,
productcode:String,quantityordered:Int,priceeach:Float,orderlinenumber:Int)

// Map every element with case class 
val order=sc.textFile("/user/hive/warehouse/hdp_datalake.db/orderdetails/")
.map(x=>{ val row=x.split("\u0001")
orderline(row(0).toInt,row(1),row(2).toInt,row(3).toFloat,row(4).toInt)
})
//order: org.apache.spark.rdd.RDD[orderline] = MapPartitionsRDD[7] at map at <console>:27

order.map(row=>(row.ordernumber,row.priceeach*row.quantityordered)).first

order.filter(row=>row.ordernumber==10425).map(row=>(row.ordernumber,row.priceeach*row.quantityordered)).foreach(println)
//pairRDD functions
//reduce by key and group by key

scala> order.filter(row=>row.ordernumber==10425).map(row=>(row.ordernumber,row.priceeach*row.quantityordered)).reduceByKey(_+_).first
res11: (Int, Float) = (10425,41623.44)

scala> order.filter(row=>row.ordernumber==10425).map(row=>(row.ordernumber,row.priceeach*row.quantityordered)).reduceByKey((x,y)=> x+y ).first
res12: (Int, Float) = (10425,41623.44)

scala> order.filter(row=>row.ordernumber==10425).map(row=>(row.ordernumber,row.priceeach*row.quantityordered)).groupByKey().first
res13: (Int, Iterable[Float]) = (10425,CompactBuffer(4996.62, 3167.67, 4126.08, 4477.16, 923.77997, 3935.4001, 4094.8801, 2956.25, 6261.71, 986.42, 3435.3901, 553.52, 1708.5599))

scala> order.filter(row=>row.ordernumber==10425).map(row=>(row.ordernumber,row.priceeach*row.quantityordered)).groupByKey().map(x=>(x._1,x._2.sum)).first
res14: (Int, Float) = (10425,41623.44)

2553  spark.read.table("orderdetails").first
2554  spark.read.table("hdp_datalake.orderdetails").first
2555  spark.read.table("hdp_datalake.orderdetails").printSchema
2556  spark.sql("select * from hdp_datalake.orderdetails").printSchema
val orderDF=spark.read.table("hdp_datalake.orderdetails")
orderDF.groupBy("ordernumber").agg(sum("priceeach")).first
orderDF.groupBy("ordernumber").sum("priceeach").first

orderDF.groupBy("ordernumber").agg(sum("priceeach")*sum("quantityordered")).where("ordernumber=10425").first

orderDF.select($"ordernumber",$"quantityordered"*$"priceeach".alias("tot_price")).show
orderDF.select($"ordernumber",$"quantityordered"*$"priceeach").withColumnRenamed("quantityordered * priceeach","tot_price").printSchema

--------------------------------------------
orderDF.select($"ordernumber",$"quantityordered"*$"priceeach").toDF("ordernumber","tot_price").printSchema

orderDF.select($"ordernumber",$"quantityordered"*$"priceeach").toDF("ordernumber","tot_price").where("ordernumber =10425 ").groupBy("ordernumber").sum("tot_price").show

orderDF.select($"ordernumber",$"quantityordered"*$"priceeach").toDF("ordernumber","tot_price").where("ordernumber =10425 ").groupBy("ordernumber").sum("tot_price").show

orderDF.select($"ordernumber",$"quantityordered"*$"priceeach").toDF("ordernumber","tot_price").where("ordernumber =10425 ").groupBy(col("ordernumber")).agg(sum("tot_price")).show


------------------------
orderDF.select($"ordernumber",$"quantityordered"*$"priceeach").toDF("ordernumber","tot_price").where("ordernumber =10425 ").groupBy(col("ordernumber")).agg(sum("tot_price").alias("tot_price")).select($"ordernumber",bround($"tot_price",2).alias("tot_price")).show
+-----------+---------+
|ordernumber|tot_price|
+-----------+---------+
|      10425| 41623.44|
+-----------+---------+

------------------------
val resDF=orderDF.select($"ordernumber",$"quantityordered"*$"priceeach").toDF("ordernumber","tot_price").where("ordernumber =10425 ").groupBy(col("ordernumber")).agg(sum("tot_price").alias("tot_price")).select($"ordernumber",bround($"tot_price",2).alias("tot_price"))


resDF.select("*","'NA' as col1").show
org.apache.spark.sql.AnalysisException: cannot resolve '`'NA' as col1`' given input columns: [ordernumber, tot_price];;

resDF.withColumn("col1",lit("NA")).show
resDF.withColumn("col1",expr("'NA'")).show


HQL and SQL functions that can be used in SQL
https://spark.apache.org/docs/2.3.0/api/sql/index.html

DF functions::
org.apache.spark.sql.functions.




spark.read.table("hdp_datalake.orderdetails").printSchema
spark.read.table("hdp_datalake.orderdetails").describe().show

spark.catalog.setCurrentDatabase("hdp_datalake")
spark.catalog.currentDatabase
spark.catalog.listColumns("orderdetails").show


scala> spark.read.table("hdp_datalake.orderdetails").createOrReplaceTempView("xyz")

scala> spark.catalog.listTables.show
+------------+------------+--------------------+---------+-----------+
|        name|    database|         description|tableType|isTemporary|
+------------+------------+--------------------+---------+-----------+
|   customers|hdp_datalake|Imported by sqoop...|  MANAGED|      false|
|orderdetails|hdp_datalake|Imported by sqoop...|  MANAGED|      false|
|         xyz|        null|                null|TEMPORARY|       true|
+------------+------------+--------------------+---------+-----------+







